from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch
import os
import warnings

# ·∫®n c·∫£nh b√°o kh√¥ng c·∫ßn thi·∫øt
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"
warnings.filterwarnings("ignore")

# Load model & tokenizer
model_name = "./finetuned_mt5"  # ho·∫∑c "./finetuned_mt5" n·∫øu m ƒë√£ train xong
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# Ch·ªçn device M1 (MPS)
device = "mps" if torch.backends.mps.is_available() else "cpu"
model = model.to(device)
model.eval()  # gi√∫p inference nhanh h∆°n, t·∫Øt dropout

print(f"üåê AI Translator (English ‚Üí Japanese) [device: {device}]")
print("Type 'exit' to quit.\n")

# H√†m d·ªãch
def translate_en_ja(text: str) -> str:
    prompt = f"translate English to Japanese: {text}"
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=256).to(device)
    with torch.no_grad():  # kh√¥ng t√≠nh gradient => nhanh h∆°n
        outputs = model.generate(
            **inputs,
            max_length=128,
            num_beams=4,  # beam search: t·ªët h∆°n greedy
            early_stopping=True,
        )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Loop t∆∞∆°ng t√°c
while True:
    text = input("Enter English text: ").strip()
    if text.lower() == "exit":
        break
    print("‚Üí", translate_en_ja(text))
